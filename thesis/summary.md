# Summary

In this thesis, i have argued for, and demonstrated the automatization of part
of a data-creation process. The findings seem to show that the system is a
useful addition to such a process, yielding largely useful recommendations with
little error. The results from chapter 7 show what kinds of procedures are most
effective, among a selection of simple preprocessing and classifier
configurations. The present system is a useful point of departure for future
information extraction, using either manual or automatic techniques.

The work presented here proceeds from the idea that data is a prerequisite for
scientific work. Facilitating more efficent data collection is an important
part of expanding the scope of testable hypotheses. This helps theory
development, by both enabling scrutiny of established theories, and perhaps
also inspiring new, more accurate ones. 

The development of more accurate theory about how ceasefires interact with
conflict is the goal of the data collection program in which this project is
embedded. Refining our knowledge about ceasefires makes it possible for
policy-makers to design better interventions and mediation strategies,
potentially migitating and preventing more conflict. 

Data is a representation of real phenomena, bridging the gap between empiricism
and theory. Importantly, data varies in quality, which is affected by the
processes with which the data is produced. Thus, specifying and running
data-collection processes with a focus on data-quality is essential. In an age
of information glut, ensuring the veracity of data is important, and but
increasingly difficult.

Data that is based on relayed information must be treated mindfully. Issues of
coverage and trust are important to consider when using such information.
Furthermore, information that is relayed through text is affected by several
stochastic processes before being expressed in symbol form; a fact that makes
the "translation" from text to data a nontrivial process. 

I believe that the findings presented here show that the application of machine
learning technology to problems of information extraction are a promising
option, allowing for a broad and comprehensive coverage of sources, and
effective handling of text-information translation. The primary issue of cost,
which is prohibitive to data-collection, is effectively remedied by the
application of automatic tools with essentially no running cost. This also
means that issues of data quality can be handled more effectively, since more
resources and focus can be put into the development of sound procedures and
algorithms rather than the trenchwork of manual data collection.
