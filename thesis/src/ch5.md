
```{R}
knitr::read_chunk('ch5_code.R')
```

```{R sentencesSetup}
```

# Data methodology

The methods used to estimate functions that map attributes to outcomes in the
following are based on supervised learning. Supervised learning is a clade of
techniques within machine learning that are distinguished by the fact that they
estimate functions on the basis of known attribute - outcome relationships.
What this means is that the outcome function is based on an observed
relationship in, preferably, many cases. This set of cases that "trains" the
model forms the basis of the models performance. 

Producing training data means manually coding as much raw data as practically
possible. Again, the quality of coding is extremely important: The validity of
outcomes generated by the resulting function is directly linked with the
validity of the manual labelling of the training data. This is because the
function is an extrapolation of the relationship observed in the training
cases: No amount of statistical trickery can remedy poorly made training data.
In addition to training, testing classifier procedures must also be done on
labelled data. This means that sufficient data must be developed, so that it
can be separated into testing and training partitions.

In this chapter I describe the origin of the data used in the following
chapters, as well as any selection and transformation procedures applied before
classification. This makes it possible to make an informed assessment of the
veracity of the results of the classification process, by examining the
extended chain of information down to the source. In addition, the validity of
the outcomes produced by the procedures in chapter seven is linked to the
validity of the coding rules used to produced the data presented here. 

## Sourcing 

The data used here was gathered from a variety of news sources through the news
aggregator Factiva. This dataset is referred to as a "corpus"; a "collection of
machine-readable texts that have been produced in a natural communicative
setting" [@pustejovsky_natural_2012 p.  19]. The foundational corpus, which
forms the basis of the data used I this thesis, is thus the entire Factiva
database, consisting of millions of news articles from over 28 000 news sources
[@oberg_gathering_2011 65].

The Factiva documents I had access to were downloaded in the context of a
coding project, specifically for the task of coding ceasefires. These documents
were pre-filtered using a search string, in order to facilitate the coding job,
and all contained references to or mentions of the concept of ceasefire. This
means that an initial pre-selection of the Factiva database was made, leaving
only articles that contained one of several phrases referencing ceasefires.
The documents were downloaded per-country, representing a second pre-selection.
This was done to assist the coders, as they worked on one country at a time.  I
selected documents from 19 countries that had been previously coded by human
coders.  This yielded 752 files containing about 100 news articles each. 

The documents were not initially machine-readable, and had to be transformed
into plain text for further processing. In addition, I chose to classify single
sentences rather than articles; I extracted the text from the documents
containing the articles, and split them up into sentences. 
Finally, I performed a third pre-selection of the data, filtering out only the
sentences that contained the word ceasefire, truce or armistice.  This resulted
in a corpus of 111 965 sentences. The distribution of sentences and ceasefires
for each country included in the analaysis can be seen in the appendix 
(table 1). 

The selection of sentences as the unit of analysis is based on an assesment of
the practical value of the predictions. Text is essentially a stream of
information, that can be unitized in many different ways. Defining a beginning
and endpoint of each unit means demarcating this stream into separate units of
analysis [@neuendorf_content_2017 72]. 

In this case, the choice of sentences rather than paragraphs or whole news
articles is based on two considerations: Firstly, the choice of whole articles
would have been too coarse: An article might perhaps mention several ceasefires
that have occurred, or might be exceedingly long, reducing the benefit of the
pre-classification. Secondly, the choice of paragraphs, which might have been
more ideal since it is a good middle-ground between the length of an article
and the precision of a sentence, is difficult from a technical standpoint. Due
to the way the articles are extracted from the source material, insufficient
formatting information is retained. This means that the next feasible level of
unitization below document-level is the sentence level with the present system
and source materials. 

### The PRIO ceasefire dataset

The selection focused on producing positive sentences started with a dataset in
production at the Peace Research Institute of Oslo (PRIO) and ETH Zürich. This
dataset contained manually coded ceasefires, covering 20 countries at the time
of writing. Manual coding was done by several research assistants at PRIO and
ETH Zürich during the fall of 2018 and winter of 2019, The assistants used the
exact same collection of PDFs that were available to me, and coded using a
simple codebook, which is included in the appendix. 

A column in the resultant dataset contains "evidence text", a sentence or text
snippet from a newspaper article. This text explicates the link between the
text source material and the coded ceasefire; this makes the sentences useful
raw material for training the algorithm to recognize text that indicates the
phenomenon of interest.. As the ceasefires were coded using the same collection
of documents that I had access to, these sentences were also essentially
retrieved from the third view. This pre-selection yielded around 500 sentences.

To pre-filter the corpus for coding negative sentences, I attempted to filter
out sentences referring to the ceasefires contained in the coded material.
I used sentences from articles about the countries that had been coded in the
partial dataset used to retrieve the positive cases. The sentences from the
articles gathered from  these documents were then filtered so that they did not
overlap with any of the known ceasefires in each country within a period of one
year. This reduced the chance of seeing the same sentences as I had seen when
coding the positive cases.

## Coding 

Supervised learning requires pre-classified data. This pre-classification will
essentially outline the decision rule which is to be mimicked by the computer,
and is vitally important to ensure good performance from the classifier, as no
amount of statistical trickery can produce a good classifier from poor training
data [@grimmer_text_2013 10]. The estimation of these procedures are described
in the following chapter. 

The sentences were manually coded as being either relevant or not irrelevant
for the coders; 1 and 0 respectively. This means that while all the sentences
used to train the model contain either the word "ceasefire", "truce" or
"armistice", the model will be trained to discern between sentences that
describe ceasefire violations, ceasefire discussions or other kinds of
circumstances, and sentences that describe ceasefire announcements and the
signing of ceasefire deals. This distinction will make further information
extraction easier, since the coders are guided towards what is considered
relevant. 

The relevant sentence were coded so that only sentences that unambiguously
describe or indicate the start of a ceasefire get a value of 1. Even sentences
that express strong intent from an actor towards signing a ceasefire, or a
commentary assessment that gives a high probability of a ceasefire, are coded
as 0. This is rule is based on the fact that the coders are attempting to
capture actual events post factum, rather than intentions, and interpretations
of circumstances. The negative sentences are in this sense negatively defined
as those sentences that do not describe or indicate an actual event;
speculation, discussion, and calls for ceasefires are all termed "irrelevant".

Some examples of actual sentences in the
training data are:

Positive:

> "Thousands of people thronged the battered streets of the coastal city
> after the ceasefire was announced"

> "The truce took effect April 1, 1988, and was to last 60 days"

> "The Nigerians agreed a city-wide ceasefire with the coup leaders on
> Monday night but said on Tuesday they were flying in reinforcements"

Negative:

> "The Organization of American States is monitoring the truce, lauded by
> its secretary general for stemming violence"

> "«They have to say 'we are ending the rebellion', not just a ceasefire,»
> Kobler told Reuters"

> "Bomani told delegates and international envoys that Mandela was still
> working "very hard" to obtain a cease-fire in Burundi"

This resulted in `r nrow(training) + nrow(holdout)` sentences, with 
`r sum(training$val) + sum(holdout$val)` 
sentences coded as positive, or "interesting", and 
`r sum(!training$val) + sum(!holdout$val)` 
coded as negative, or "uninteresting". 10 percent of these sentences were put
aside as a "holdout" dataset, and reserved for a final evaluation of the
classifier, while 90 percent will be used to understand the characteristics of
the raw text, and to estimate and preliminarily evaluate methods of
classification. From this set 

## Description

A description of the training data helped guide the process of specifying the
classification scheme. This is helpful, because processing the text in
different ways might be a powerful way to improve classifier performance, but
not in all cases.  Different sources of text varies in different ways: The
range of vocabulary, the length of paragraphs and sentences, and the use of
words might differ between sources and genres. Therefore, intimate knowledge of
the characteristics of the text material is important to be able to reason
about techniques and procedure

One of the most important questions is perhaps; are the texts belonging to
either class sufficiently different to make classification viable? If so, in
what way do they differ, and how can the difference between the sets be used
best to construct the classifier. The present material has some important
characteristics for designing the classifier:

```{R sentenceLength,  include = TRUE} 
```

The length is interesting, because it determines the chance that tokens are
observed more than once for each unit. In the newspaper sentences, while some
sentences are quite long, unique words very rarely appear more than once; the
mean number of times each word appears in each sentence is `r round(wordPerDoc,
digits = 4)`, meaning there is only a slim chance of observing a given word
more than once. This has implications for how the sentences are vectorized.

In addition to giving a description of the raw text as a whole, it is also
important to make sure that the text associated with each class varies. This is
important, because this variation will be used to train the classifier to
recognize text as belonging to each class. As shown here, there seems to be
some useful variation between the classes: relevant sentences very often
contain the word "signed" and "agreed", while these words are not present as
often in the irrelevant sentences. 

This plot shows words word frequencies for each group of texts, as a percentage
of total words. Since, as mentioned above, the sentences were selected as
containing the words "ceasefire", "truce" and "armistice", these words are
excluded from the plot. In addition, i excluded stopwords from the plot.
Stopwords are grammatical words like prepositions and pronouns that occur very
often in regular text [@bird_natural_2009 60].  The reason for excluding them
here is that i am interested in the differences in the more semantically loaded
words, like verbs and nouns. The exclusion of stopwords before classification
might also improve classifier performance, an assumption which is tested in
chapter 7.  Note that both uppercase and lowercase words are shown; these count
as discrete tokens, unless the text is normalized by lowercasing all words.

```{R wordFreq, include = TRUE} 
```


